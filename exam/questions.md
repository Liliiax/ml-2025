#### 1. Основные этапы жизненного цикла модели машинного обучения

> _**CRISP-DM**_ (Cross-Industry Standard Process for Data Mining) — это общий стандартный процесс, используемый для организации и управления проектами в области анализа данных и добычи данных

<img src="https://github.com/Liliiax/ml-2025/blob/50bfcf55ace155b888f3b306adf7ed8edc1b1638/exam/src/%D0%96%D0%B8%D0%B7%D0%BD%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9_%D1%86%D0%B8%D0%BA%D0%BB_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8F.jpeg" width="50%"/>
#### 2. Обучение с учителем и обучение без учителя, проблемы, связанные с обучением с учителем и без учителя
#### 3. Постановка и решение задачи восстановления линейной регрессии
#### 4. Процесс минимизации квадратичного функционала эмпирического риска
#### 5. Конструирование новых признаков на основе существующих
#### 6. Метрики для оценки качества регрессионных моделей
#### 7. Явления недообучения и переобучения, методы отбора признаков
#### 8. Регуляризация, основные отличия между LASSO и Ridge регрессией
#### 9. Сингулярное разложение матрицы признаков и его построение
#### 10. Связь спектра матрицы признаков с переобучением модели
#### 11. Некорректная задача приближения по минимуму функционала ошибки с использованием псевдообратной матрицы
#### 12. Этапы решения задачи безусловной минимизации функционала ошибки на основе методов спуска
#### 13. Оценка сходимости градиентного спуска для гладких выпуклых функций
#### 14. Назначение и реализация стохастического градиентного спуска
#### 15. Модели для предсказания дискретной величины
#### 16. Логистическая регрессия и ее принципы
#### 17. Задача условной оптимизации в методе опорных векторов
#### 18. Обобщение метода опорных векторов с заменой функции-ядра
#### 19. Методы снижения размерности признакового пространства
#### 20. Линейный метод главных компонент (PCA)
#### 21. Обобщение линейного метода главных компонент (KernelPCA)
#### 22. Алгоритмы кластеризации k-средних и агломеративные методы
#### 23. Алгоритм кластеризации DBSCAN
#### 24. Настройка гиперпараметров моделей машинного обучения
#### 25. Безградиентные методы оптимизации




Выделяет `sizeof(T)` байт памяти и конструирует объект

```cpp
T* ptr = new T{};
```
Удаляет объект и освобождает память

```cpp
delete ptr;
```
Точно так же можно аллоцировать массивы объектов
Выделяет `sizeof(T) * 10` байт памяти и конструирует 10 объектов

```cpp
T* arr = new T[10];
```

```cpp
delete[] arr;
```

## Ручное конструирование объектов

С помощью оператора `new` аллоцируем нужное количество байт, он возвращает нам указатель типа `void*` _(его разыменовать не можем, обычно используется для определения какого-то адреса в памяти)_

Далее уже конструируем в этом адресе объект типа `T` _(placement new)_

> если сделать +=1 к указателю типа void*, то программа не скомпилируется, так
> как sizeof(void) довольно странная вещь, в с++ такого не разрешают


```cpp
void* ptr_raw = ::operator new(sizeof(T));
T* ptr = new (ptr_raw) T();
```

Чтобы по адресу `ptr` удалить объект типа `T`, вызываем явный деструктор, и потом освобождаем память с помощью `delete`

```cpp
ptr-> ~T();
::operator delete(ptr)
```




