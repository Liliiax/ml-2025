### 1. Основные этапы жизненного цикла модели машинного обучения

> _**CRISP-DM**_ (Cross-Industry Standard Process for Data Mining) — это общий стандартный процесс, используемый для организации и управления проектами в области анализа данных и добычи данных

<img src="https://github.com/Liliiax/ml-2025/blob/50bfcf55ace155b888f3b306adf7ed8edc1b1638/exam/src/%D0%96%D0%B8%D0%B7%D0%BD%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9_%D1%86%D0%B8%D0%BA%D0%BB_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8F.jpeg" width="50%"/>

### 2. Обучение с учителем и обучение без учителя, проблемы, связанные с обучением с учителем и без учителя

#### _2.1 Обучение с учителем:_
> Обучение с учителем состоит из двух подкатегорий: **классификации и регрессии**.
> 
Этот подход подразумевает обучение алгоритма МО на _размеченных наборах данных_. Для каждого примера в обучающем наборе алгоритм знает, какой результат является правильным. Он использует эти знания, чтобы попытаться обобщить их на новые примеры, которые он никогда раньше не видел. Применяя эту информацию, модель может постепенно обучаться и повышать свою точность.

Цель этого метода заключается в умении модели устанавливать связь между выходными и входными данными. Она тренируется, итеративно составляя прогнозы на входах и корректируя свои параметры для получения верного ответа. 

#### Преимущества обучения с учителем:
* высокая точность прогнозирования
* широкий спектр применений
* высокая степень интерпретируемости
* контроль процесса обучения
* оценка производительности алгоритма
* инкрементальное обучение

#### Недостатки:
* проблема доступности данных
* зависимость надёжности прогнозов и эффективности модели от качества и согласованности разметки
* зависимость производительности модели от правильно выбранных входных переменных
* сложность масштабирования
* ограниченность закономерностей, поскольку они только в пределах предоставленных наборов обучающих данных
* высокая вычислительная стоимость.

#### _2.2 Обучение без учителя:_
> Модели обучения без учителя группируют данные и используются для решения трёх основных задач: **кластеризация, ассоциация, сокращение размерности**.
> 
Это подход, применяемый для обнаружения базовой структуры данных. Алгоритмы обучения без учителя не требуют отображения входов и выходов. Обычно они используются для выявления существующих закономерностей в данных, так что экземпляры группируются без необходимости в метках (короче нет правильных ответов `^_^`) Предполагается, что экземпляры, которые попадают в одну группу, имеют схожие характеристики.
Этот метод чаще всего используется в таких сценариях, как сегментация клиентов, обнаружение аномалий, анализ потребительской корзины, кластеризация документов, анализ социальных сетей, сжатие изображений.

#### Преимущества обучения без учителя:
* не нужны размеченные наборы данных
* выявляются скрытые закономерности
* сокращаются размерности
* выявляются аномалии и выбросы в представленных данных
* повышается экономическая эффективность

#### Недостатки:
* трудности в интерпретации результатов из-за отсутствия меток
* нет чётких метрик
* ресурсоёмкость
* проблемы переобучения
* зависимость от качества используемых признаков.

#### Как правильно выбрать и немного про частичное обучение:

Обучение с учителем используется чаще, чем без него, потому что оно точнее и эффективное. В свою очередь, обучение без учителя можно использовать для данных, которые не размечены, что часто встречается. Также его можно применять для поиска скрытых закономерностей в данных, которые модели обучения с учителем не смогут обнаружить. Контролируемое обучение проблематично для классификации больших данных, но полученные результаты будут максимально точными. Алгоритмы неконтролируемого обучения легче обрабатывают большие данные в режиме реального времени, но конечные результаты менее точны.

Есть золотая середина, известная как обучение с частичным привлечением учителя. Здесь используется набор обучающих данных как с размеченными, так и с неразмеченными данными. Это полезно, когда трудно извлечь соответствующие функции из больших объёмов. Например, такой алгоритм можно использовать для набора с миллионами изображений, из которых размечены только несколько тысяч.
  
### 3. Постановка и решение задачи восстановления линейной регрессии
> **Линейная регрессия** (англ. linear regression) — метод восстановления зависимости одной (объясняемой, зависимой) переменной y от другой или нескольких других переменных (факторов, регрессоров, независимых переменных) x с линейной функцией зависимости. Данный метод позволяет предсказывать значения зависимой переменной y по значениям независимой переменной x

#### Математическая модель: 
```
Дано: 
- f₁(x), ..., fₙ(x) — числовые признаки
- Модель: f(x,α) = α₁f₁(x) + ... + αₙfₙ(x)
- α ∈ ℝⁿ — вектор параметров (вектор весов)
- Обучающая выборка: (xᵢ, yᵢ), где i = 1...l
- xᵢ ∈ X = ℝⁿ — объекты 
- yᵢ ∈ Y = ℝ — целевые значения


Постановка задачи:
Q(α) = ||f(x,α) - y||² → min α

- f(x,α) — предсказания модели для всех объектов
- f(x,α) - y — вектор ошибок (разница между предсказаниями и реальными значениями)
- ||f(x,α) - y||² — сумма квадратов ошибок (SSE)
- Нужно найти такой вектор весов α, который минимизирует эту сумму
```

#### Решение: 
```

<img src="https://github.com/Liliiax/ml-2025/blob/50bfcf55ace155b888f3b306adf7ed8edc1b1638/exam/src/%D0%96%D0%B8%D0%B7%D0%BD%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9_%D1%86%D0%B8%D0%BA%D0%BB_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8F.jpeg" width="50%"/>
```


### 4. Процесс минимизации квадратичного функционала эмпирического риска
#### 5. Конструирование новых признаков на основе существующих
#### 6. Метрики для оценки качества регрессионных моделей
#### 7. Явления недообучения и переобучения, методы отбора признаков
#### 8. Регуляризация, основные отличия между LASSO и Ridge регрессией
#### 9. Сингулярное разложение матрицы признаков и его построение
#### 10. Связь спектра матрицы признаков с переобучением модели
#### 11. Некорректная задача приближения по минимуму функционала ошибки с использованием псевдообратной матрицы
#### 12. Этапы решения задачи безусловной минимизации функционала ошибки на основе методов спуска
#### 13. Оценка сходимости градиентного спуска для гладких выпуклых функций
#### 14. Назначение и реализация стохастического градиентного спуска
#### 15. Модели для предсказания дискретной величины
#### 16. Логистическая регрессия и ее принципы
#### 17. Задача условной оптимизации в методе опорных векторов
#### 18. Обобщение метода опорных векторов с заменой функции-ядра
#### 19. Методы снижения размерности признакового пространства
#### 20. Линейный метод главных компонент (PCA)
#### 21. Обобщение линейного метода главных компонент (KernelPCA)
#### 22. Алгоритмы кластеризации k-средних и агломеративные методы
#### 23. Алгоритм кластеризации DBSCAN
#### 24. Настройка гиперпараметров моделей машинного обучения
#### 25. Безградиентные методы оптимизации




Выделяет `sizeof(T)` байт памяти и конструирует объект

```cpp
T* ptr = new T{};
```
Удаляет объект и освобождает память

```cpp
delete ptr;
```
Точно так же можно аллоцировать массивы объектов
Выделяет `sizeof(T) * 10` байт памяти и конструирует 10 объектов

```cpp
T* arr = new T[10];
```

```cpp
delete[] arr;
```

## Ручное конструирование объектов

С помощью оператора `new` аллоцируем нужное количество байт, он возвращает нам указатель типа `void*` _(его разыменовать не можем, обычно используется для определения какого-то адреса в памяти)_

Далее уже конструируем в этом адресе объект типа `T` _(placement new)_

> если сделать +=1 к указателю типа void*, то программа не скомпилируется, так
> как sizeof(void) довольно странная вещь, в с++ такого не разрешают


```cpp
void* ptr_raw = ::operator new(sizeof(T));
T* ptr = new (ptr_raw) T();
```

Чтобы по адресу `ptr` удалить объект типа `T`, вызываем явный деструктор, и потом освобождаем память с помощью `delete`

```cpp
ptr-> ~T();
::operator delete(ptr)
```




